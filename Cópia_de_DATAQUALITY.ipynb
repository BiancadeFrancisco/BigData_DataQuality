{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj6ptJMD3SZv1ADax3NazA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancadeFrancisco/BigData_DataQuality2/blob/main/C%C3%B3pia_de_DATAQUALITY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NQWnEl8-n_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6ee216-03dd-42e7-e39c-3bbca4f76128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.3\n",
            "  Downloading pyspark-3.0.3.tar.gz (209.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.1/209.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.0.3)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.3-py2.py3-none-any.whl size=209435950 sha256=4301dab9f6e0760a50c7a8e42a43b5a63b648e02c59b297446254d06d2e4ca44\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/50/14/79047c3c171b701e591d287b78a201214d9c8e0b93fef64458\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.3\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "https://b46e-35-194-143-42.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Instalar a vesão 3.0.3 do PySpark\n",
        "!pip install pyspark==3.0.3\n",
        "\n",
        "# Instalar o NGROK\n",
        "!wget -qnc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n -q ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Autenticar a sessão do SparkUI com NGROK\n",
        "!./ngrok authtoken 2KBeQEmmd1YNlQ86GGKf3KFOkb3_6sQH7JEnvEhDxwn9A7WnT\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!sleep 10\n",
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydeequ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbiK_ZlO_Fsv",
        "outputId": "79b56afc-c4cb-4365-ed61-54c5d660f3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydeequ\n",
            "  Downloading pydeequ-1.1.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from pydeequ) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from pydeequ) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.0->pydeequ) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\n",
            "Installing collected packages: pydeequ\n",
            "Successfully installed pydeequ-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "SUCKCVvHkR2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -- version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdJ3NDnJlFwD",
        "outputId": "908ec982-978e-4fcb-af6a-98575ad4ed2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/version': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Defina a versão do Spark que você deseja usar\n",
        "os.environ['SPARK_VERSION'] = '3.0'\n",
        "\n",
        "# Agora, importe o PyDeequ\n",
        "from pydeequ import *"
      ],
      "metadata": {
        "id": "4KJbgzgIkTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydeequ"
      ],
      "metadata": {
        "id": "GMlbwpADLNLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .config('spark.ui.port', '4050')\n",
        "      .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
        "      .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
        "      .appName(\"SparkSQL\")\n",
        "      .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "id": "sJ0O7kRj_HU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType())\n",
        "])\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('remetente', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('categoria', StringType()),\n",
        "    StructField('transaction_date', StringType()),\n",
        "    StructField('fraude', IntegerType())\n",
        "])\n",
        "\n",
        "caminho_json = '/content/case_final.json'\n",
        "\n",
        "df = spark.read.json(\n",
        "    caminho_json,\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"yyyy-MM-dd HH:mm:ss\"\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "      'destinatario_nome', col('destinatario').getField('nome')\n",
        "    ).withColumn(\n",
        "      'destinatario_banco', col('destinatario').getField('banco')\n",
        "    ).withColumn(\n",
        "      'destinatario_tipo', col('destinatario').getField('tipo')\n",
        "    ).withColumn(\n",
        "      'remetente_nome', col('remetente').getField('nome')\n",
        "    ).withColumn(\n",
        "      'remetente_banco', col('remetente').getField('banco')\n",
        "    ).withColumn(\n",
        "      'remetente_tipo', col('remetente').getField('tipo')\n",
        ").drop('remetente', 'destinatario')"
      ],
      "metadata": {
        "id": "xF7ThD1M_H6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.analyzers import AnalysisRunner, AnalyzerContext, ApproxCountDistinct, Completeness, Compliance, Mean, Size\n",
        "\n",
        "\n",
        "analysisResult = (\n",
        "    AnalysisRunner(spark).onData(df)\n",
        "    .addAnalyzer(Size()) # análise para verificar o tamanho do arquivo\n",
        "    .addAnalyzer(Completeness('id_transacao')) # análise para verificar a completude da coluna id_transacao\n",
        "    .addAnalyzer(Compliance(\"valor\", \"valor > 0\")) # vai analisar se a coluna valor atende o querisito imposto: > 0\n",
        "    .run()\n",
        ")\n"
      ],
      "metadata": {
        "id": "W8gbvsNU_O2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysisResult"
      ],
      "metadata": {
        "id": "7ex-lfvQ_PZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2201f2-8512-4b85-9ef2-06c1a332f69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "JavaObject id=o138"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)"
      ],
      "metadata": {
        "id": "0yA2L6R1_Qwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysisResult_df.show()\n",
        "\n",
        "# resultados: temos 100 mil linhas no dataset, a coluna id está completa com 100% dos valores e na coluna valor ele tem 99,97% de chance dos números serem maiores que 0."
      ],
      "metadata": {
        "id": "4_A9tfdr_Sb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bb6c3c-1883-4dfa-bce3-518079f22a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+--------+\n",
            "| entity|    instance|        name|   value|\n",
            "+-------+------------+------------+--------+\n",
            "|Dataset|           *|        Size|100000.0|\n",
            "| Column|id_transacao|Completeness|     1.0|\n",
            "| Column|       valor|  Compliance| 0.99972|\n",
            "+-------+------------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CASO EU QUEIRA UMA SUGESTÃO DO PYDEEQU SOBRE ALGUMAS ANÁLISES QUE SERIAM INTERESSANTES EU FAZER COM OS DADOS DO MEU DATASET:\n",
        "\n",
        "from pydeequ.suggestions import ConstraintSuggestionRunner, DEFAULT\n",
        "\n",
        "suggestionResult = ConstraintSuggestionRunner(spark).onData(df).addConstraintRule(DEFAULT()).run()\n",
        "\n"
      ],
      "metadata": {
        "id": "QJBB_v-R_SXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNÇÃO PARA QUE ELE MESMO LISTE AS SUGESTÕES DE ANÁLISES E OS CÓDIGOS DELES\n",
        "\n",
        "for sugg in suggestionResult['constraint_suggestions']:\n",
        "  print(f\"Sugestao de Constraint: \\'{sugg['column_name']}\\': {sugg['description']}\")\n",
        "  print(f\"PySpark Code: {sugg['code_for_constraint']}\\n\")"
      ],
      "metadata": {
        "id": "sHXS-mdJ_SSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed73eb2-1d53-4809-9d2e-5121d2dac2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugestao de Constraint: 'destinatario_nome': 'destinatario_nome' is not null\n",
            "PySpark Code: .isComplete(\"destinatario_nome\")\n",
            "\n",
            "Sugestao de Constraint: 'remetente_nome': 'remetente_nome' has value range 'Jonathan Gonsalves'\n",
            "PySpark Code: .isContainedIn(\"remetente_nome\", [\"Jonathan Gonsalves\"])\n",
            "\n",
            "Sugestao de Constraint: 'remetente_nome': 'remetente_nome' is not null\n",
            "PySpark Code: .isComplete(\"remetente_nome\")\n",
            "\n",
            "Sugestao de Constraint: 'id_transacao': 'id_transacao' is not null\n",
            "PySpark Code: .isComplete(\"id_transacao\")\n",
            "\n",
            "Sugestao de Constraint: 'id_transacao': 'id_transacao' has no negative values\n",
            "PySpark Code: .isNonNegative(\"id_transacao\")\n",
            "\n",
            "Sugestao de Constraint: 'id_transacao': 'id_transacao' is unique\n",
            "PySpark Code: .isUnique(\"id_transacao\")\n",
            "\n",
            "Sugestao de Constraint: 'remetente_banco': 'remetente_banco' has value range 'BTG'\n",
            "PySpark Code: .isContainedIn(\"remetente_banco\", [\"BTG\"])\n",
            "\n",
            "Sugestao de Constraint: 'remetente_banco': 'remetente_banco' is not null\n",
            "PySpark Code: .isComplete(\"remetente_banco\")\n",
            "\n",
            "Sugestao de Constraint: 'categoria': 'categoria' has value range 'transferencia', 'alimentacao', 'vestuario', 'saude', 'lazer', 'educacao', 'outros', 'presentes', 'transporte'\n",
            "PySpark Code: .isContainedIn(\"categoria\", [\"transferencia\", \"alimentacao\", \"vestuario\", \"saude\", \"lazer\", \"educacao\", \"outros\", \"presentes\", \"transporte\"])\n",
            "\n",
            "Sugestao de Constraint: 'categoria': 'categoria' is not null\n",
            "PySpark Code: .isComplete(\"categoria\")\n",
            "\n",
            "Sugestao de Constraint: 'categoria': 'categoria' has value range 'transferencia', 'alimentacao', 'vestuario', 'saude', 'lazer', 'educacao', 'outros', 'presentes' for at least 90.0% of values\n",
            "PySpark Code: .isContainedIn(\"categoria\", [\"transferencia\", \"alimentacao\", \"vestuario\", \"saude\", \"lazer\", \"educacao\", \"outros\", \"presentes\"], lambda x: x >= 0.9, \"It should be above 0.9!\")\n",
            "\n",
            "Sugestao de Constraint: 'remetente_tipo': 'remetente_tipo' has value range 'PF'\n",
            "PySpark Code: .isContainedIn(\"remetente_tipo\", [\"PF\"])\n",
            "\n",
            "Sugestao de Constraint: 'remetente_tipo': 'remetente_tipo' is not null\n",
            "PySpark Code: .isComplete(\"remetente_tipo\")\n",
            "\n",
            "Sugestao de Constraint: 'chave_pix': 'chave_pix' has value range 'email', 'cpf', 'celular', 'aleatoria'\n",
            "PySpark Code: .isContainedIn(\"chave_pix\", [\"email\", \"cpf\", \"celular\", \"aleatoria\"])\n",
            "\n",
            "Sugestao de Constraint: 'chave_pix': 'chave_pix' is not null\n",
            "PySpark Code: .isComplete(\"chave_pix\")\n",
            "\n",
            "Sugestao de Constraint: 'fraude': 'fraude' has value range '0', '1'\n",
            "PySpark Code: .isContainedIn(\"fraude\", [\"0\", \"1\"])\n",
            "\n",
            "Sugestao de Constraint: 'fraude': 'fraude' is not null\n",
            "PySpark Code: .isComplete(\"fraude\")\n",
            "\n",
            "Sugestao de Constraint: 'fraude': 'fraude' has no negative values\n",
            "PySpark Code: .isNonNegative(\"fraude\")\n",
            "\n",
            "Sugestao de Constraint: 'destinatario_tipo': 'destinatario_tipo' has value range 'PJ', 'PF'\n",
            "PySpark Code: .isContainedIn(\"destinatario_tipo\", [\"PJ\", \"PF\"])\n",
            "\n",
            "Sugestao de Constraint: 'destinatario_tipo': 'destinatario_tipo' is not null\n",
            "PySpark Code: .isComplete(\"destinatario_tipo\")\n",
            "\n",
            "Sugestao de Constraint: 'valor': 'valor' is not null\n",
            "PySpark Code: .isComplete(\"valor\")\n",
            "\n",
            "Sugestao de Constraint: 'valor': 'valor' has no negative values\n",
            "PySpark Code: .isNonNegative(\"valor\")\n",
            "\n",
            "Sugestao de Constraint: 'transaction_date': 'transaction_date' is not null\n",
            "PySpark Code: .isComplete(\"transaction_date\")\n",
            "\n",
            "Sugestao de Constraint: 'transaction_date': 'transaction_date' is unique\n",
            "PySpark Code: .isUnique(\"transaction_date\")\n",
            "\n",
            "Sugestao de Constraint: 'destinatario_banco': 'destinatario_banco' has value range 'XP', 'BTG', 'Nubank', 'Itau', 'Caixa', 'C6', 'Bradesco'\n",
            "PySpark Code: .isContainedIn(\"destinatario_banco\", [\"XP\", \"BTG\", \"Nubank\", \"Itau\", \"Caixa\", \"C6\", \"Bradesco\"])\n",
            "\n",
            "Sugestao de Constraint: 'destinatario_banco': 'destinatario_banco' is not null\n",
            "PySpark Code: .isComplete(\"destinatario_banco\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
        "from pydeequ.verification import VerificationResult, VerificationSuite\n",
        "\n",
        "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
        "error = Check(spark, CheckLevel.Error, \"Error\")"
      ],
      "metadata": {
        "id": "4Fi3KWLk_f5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult = (\n",
        "    VerificationSuite(spark)\n",
        "      .onData(df)\n",
        "      .addCheck(\n",
        "        check.hasDataType(\"id_transacao\", ConstrainableDataTypes.Integral)\n",
        "        .isNonNegative(\"id_transacao\")\n",
        "        .isComplete(\"id_transacao\")\n",
        "      )\n",
        "  .run()\n",
        ")"
      ],
      "metadata": {
        "id": "H-GnpXEK_f0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
        "checkResult_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "KDpISK_M_fvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfae1591-e265-4e8f-b42d-03a1b733c3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
            "|check       |check_level|check_status|constraint                                                                                                                  |constraint_status|constraint_message|\n",
            "+------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
            "|Review Check|Warning    |Success     |AnalysisBasedConstraint(DataType(id_transacao,None),<function1>,Some(<function1>),None)                                     |Success          |                  |\n",
            "|Review Check|Warning    |Success     |ComplianceConstraint(Compliance(id_transacao is non-negative,COALESCE(CAST(id_transacao AS DECIMAL(20,10)), 0.0) >= 0,None))|Success          |                  |\n",
            "|Review Check|Warning    |Success     |CompletenessConstraint(Completeness(id_transacao,None))                                                                     |Success          |                  |\n",
            "+------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testar um erro para visualizar como o modelo responde:\n",
        "\n",
        "\n",
        "checkResult = (\n",
        "    VerificationSuite(spark)\n",
        "      .onData(df)\n",
        "      .addCheck(\n",
        "        error\n",
        "          .isContainedIn(\"remetente_tipo\", [\"CNPJ\"])\n",
        "      )\n",
        "  .run()\n",
        ")"
      ],
      "metadata": {
        "id": "xgHkMPDZ_jv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
        "checkResult_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "QWNTC9tv_jti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e69d329-22c3-4ac9-dc77-dc79861878f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------+\n",
            "|check|check_level|check_status|constraint                                                                                                                      |constraint_status|constraint_message                                  |\n",
            "+-----+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------+\n",
            "|Error|Error      |Error       |ComplianceConstraint(Compliance(remetente_tipo contained in CNPJ,`remetente_tipo` IS NULL OR `remetente_tipo` IN ('CNPJ'),None))|Failure          |Value: 0.0 does not meet the constraint requirement!|\n",
            "+-----+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ESSE ASSUNTO É MUITO RELEVANTE E ESTÁ EM ALTA, VERIFICAR DOCUMENTAÇÃO E OUTROS EXEMPLOS E PARÂMETROS."
      ],
      "metadata": {
        "id": "GMYRAcx5i2Lz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}